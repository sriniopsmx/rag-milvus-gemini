{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c14c6f-36cb-4c49-80f3-2f611a7a3b0e",
   "metadata": {},
   "source": [
    "### Program to Get Context for an Issue\n",
    "End result is a Query that can be send to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5637e59b-ee9e-47f8-ba07-1963d3aee37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srini/AI-ML/testenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Mivus\n",
    "from pymilvus import MilvusClient, model,connections, db\n",
    "\n",
    "# Huggingface\n",
    "from datasets import Dataset, load_dataset # For loading\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "#import PIL.Image\n",
    "import os\n",
    "import streamlit as st "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec59949-7e26-4004-bc0e-7e28d43d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "milHost=\"192.168.1.44\"\n",
    "milURI='http://192.168.1.44:19530'\n",
    "milDBname=\"milvus_demo\"\n",
    "milCollection=\"demo_collection\"\n",
    "modelDimension= 384 # 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82bacf2-165a-4826-b1f4-4e3671435908",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyA0TpSBwP8EuxJoJf8dIqgkXU9b5QDDh9A\" ) #os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8912895-6436-47d3-9fc3-77aef2979eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srini/AI-ML/testenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This is model being used for RAG, not for inference\n",
    "MODEL =  \"sentence-transformers/all-MiniLM-L6-v2\"  # Name of model from HuggingFace Models\n",
    "INFERENCE_BATCH_SIZE = 64  # Batch size of model inference, may be used for uploading the docs into Milvus?\n",
    "\n",
    "# Load tokenizer & model from HuggingFace Hub\n",
    "ragTokenizer = AutoTokenizer.from_pretrained(MODEL) #This will be used to tokenize the inputs and uploading into Milvus\n",
    "ragModel = AutoModel.from_pretrained(MODEL) # This should be used for finding relavent docs\n",
    "ragData = pd.DataFrame(columns=['Relevance', 'Issue','Context'])\n",
    "ragPrompt=\"Please wait\"\n",
    "changeStr=\". Nothing has apparently changed.\"\n",
    "changeHappened = False\n",
    "answer = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fea024d7-e5a8-4d6b-9865-ae5d6edfac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am tokenizing only the incident description\n",
    "# data.map uses this function to \n",
    "def encode_text(batch):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = ragTokenizer(\n",
    "        batch[\"Incident Description\"], padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = ragModel(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    token_embeddings = model_output[0]\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    sentence_embeddings = torch.sum(\n",
    "        token_embeddings * input_mask_expanded, 1\n",
    "    ) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    batch[\"description_embedding\"] = torch.nn.functional.normalize(\n",
    "        sentence_embeddings, p=2, dim=1\n",
    "    )\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a39e41bf-3e7c-4a91-8fb6-f25a741bfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(basePrompt, changeHappened):\n",
    "#basePrompt= \"Spinnaker deployments are failing with access denied. No configurations have changed since it was last successful.\"\n",
    "    questions = {\n",
    "        \"Incident Description\": [\n",
    "            # \"account access denied\",\n",
    "            # \"deployments are very slow\"\n",
    "            # \"sky is falling\",\n",
    "           basePrompt,\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Generate question embeddings\n",
    "    question_embeddings = [v.tolist() for v in encode_text(questions)[\"description_embedding\"]]\n",
    "    milvus_client = MilvusClient(milURI)\n",
    "    # Now we search for our Incident description in Milvus\n",
    "    search_results = milvus_client.search(\n",
    "        collection_name=milCollection,\n",
    "        data=question_embeddings,\n",
    "        limit=5,  # How many search results to output\n",
    "        output_fields=[\"Responses\", \"Context\"],  # Include these fields in search results\n",
    "    )\n",
    "    \n",
    "    # Prepare PROMPT\n",
    "    if changeHappened :\n",
    "       ragPrompt=\"You an expert in Open Source Spinnaker. A user is facing this issue:\" + basePrompt + \". Here are some responses to similar issues:\"\n",
    "    else:    \n",
    "       ragPrompt=\"You an expert in Open Source Spinnaker. A user is facing this issue:\" + basePrompt + changeStr + \". Here are some responses to similar issues:\"\n",
    "    for q, res in zip(questions[\"Incident Description\"], search_results):\n",
    "        #print(basePrompt)\n",
    "        #print(\"\\ncontext:\\n\")\n",
    "        for r in res:\n",
    "            ans= r[\"entity\"][\"Responses\"]\n",
    "            ctxt = r[\"entity\"][\"Context\"]\n",
    "            score = r[\"distance\"]\n",
    "            #ragData.add({'Relevance':ans, 'Issue':ctxt,'Context':score})\n",
    "            ragData.loc[len(ragData.index)] = [ score, ans, ctxt ] \n",
    "            if score > 0.6:\n",
    "                #print(f\"{ans}\\n\") \n",
    "                ragPrompt += ans + \"\\n\"\n",
    "                if len(ctxt) > 10 :\n",
    "                    ragPrompt += ctxt + \"\\n\"\n",
    "    return model.generate_content(ragPrompt), ragPrompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fadaf5ec-da95-46f1-b8c4-7401e86061e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_page_config(layout=\"wide\")\n",
    "colA, colB, colC = st.columns([.50, .10, .40])\n",
    "response=\"\"\n",
    "with colA:\n",
    "    basePrompt = st.text_input(\"Please enter the user issue, ensure that 'spinnaker' word is present:\", value=\"\", key=\"prompt\")\n",
    "with colB:\n",
    "    changeHappened = st.checkbox(\"Something changed?\", value=False, key=\"change\")\n",
    "    if st.button(\"Press\", key=\"button\"):\n",
    "        response, ragPrompt = get_response(basePrompt, changeHappened)\n",
    "        print(\"RAGPrompt:\", ragPrompt)\n",
    "        answer = response.text\n",
    "with colA:\n",
    "    st.markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e0f9a97-7031-4264-ab78-044be550058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.append({'Relevance':0.6, 'Issue':'hello','Context':'newinfo'})\n",
    "with colC:\n",
    "  st.subheader(\"Related Items found (top 3 with score > 0.6 are considered)\")\n",
    "  st.table(ragData)\n",
    "  st.write(\"Final Prompt:\" + ragPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47c5b799-e785-4686-b84e-83c4eb5083ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q, res in zip(questions[\"Incident Description\"], search_results):\n",
    "#     print(\"Incident Description:\", q)\n",
    "    \n",
    "#     for r in res:\n",
    "#         ans= r[\"entity\"][\"Responses\"]\n",
    "#         ctxt = r[\"entity\"][\"Context\"]\n",
    "#         score = r[\"distance\"]\n",
    "#         if score > 0.4:\n",
    "#             print (\"Here are the selections\\n\")\n",
    "#             print(f\"Ans: {ans}\\n\") \n",
    "#             print(f\"\\tContext: {ctxt}\\n\")\n",
    "#             print(f\"\\tScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82b73d37-2282-45ea-b1e8-4be0fd160ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ragPrompt)\n",
    "# resp = get_response(ragPrompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130901a-7ddb-4d4d-8cd8-57950ec08eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
