{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4c1e77-5016-4289-a894-d691d1e27e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "#model_name='nbeerbower/gemma2-gutenberg-9B'\n",
    "model_name='google/gemma-2-2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "198dd0e0-2659-4aa2-96aa-484fbb1e1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM # TFAutoModelForQuestionAnswering # , TFAutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2717a7-0b85-4ecd-9d33-0730f37b519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c375dfb8734077ac67c18c7f9bbd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cfe0782d434b6aa79237b63d4abae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf9519f436e442eb7a07c36d5e8821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5202be87c44102aa8009bc0e22c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b10d7d104644e3fbd56895d13bd2a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e387f9fb60459f914976f451cf2e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0525a3d8e04027a25cf2413bcf5583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fc18d83d634481a1b6c6b2f14c4d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b8f6482faf443f83cd12e30833eed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d02cc9aa5874822a2d2b2f1acee8c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dd37410e614baf9801cfc44d08061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265d3cd35c44aa5b90f175d5371c42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=\"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\")\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c6f7d8a-2296-4e74-abe8-59e4e459b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "# classifier = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\",  # replace with \"mps\" to run on a Mac device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea54c9ed-bb57-4e40-a79f-f9f05bf5dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1722905711.4529884\n",
      "Time Taken(seconds): 127.231294\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "startTime = datetime.datetime.now()\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "outputs = pipe(messages, max_new_tokens=256)\n",
    "print(time.time())\n",
    "endTime = datetime.datetime.now()\n",
    "tSpent = endTime - startTime\n",
    "print(\"Time Taken(seconds):\", tSpent.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3befc8f9-484c-4b3a-a214-7f4c7c10c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b0d2c8-0dc6-4c1e-ab1d-b62410823a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'user', 'content': 'Who are you? Please, answer in pirate-speak.'}, {'role': 'assistant', 'content': '<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<'}]}]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59713840-e7cc-45ca-aaa1-f1e55694f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write me a poem about Machine Learning.<|im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end|>\n",
      "<im_end\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cpu\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8c6816-0be2-4f6b-83f7-f1b5c0241fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=a12e4911b904364bddba5199541ff963cec837732261f849e336b7e1e0bdd1de\n",
      "  Stored in directory: /home/srini/.cache/pip/wheels/ba/03/bb/7a97840eb54479b328672e15a536e49dc60da200fb21564d53\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install GPUtil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8c1a251-c1a0-41a6-9c30-6fd8d0f6836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Why is the Sky blue?\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9019f325-baa0-413c-84fa-7899f338ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken(seconds): 133.93873\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "endTime = datetime.datetime.now()\n",
    "tSpent = endTime - startTime\n",
    "print(\"Time Taken(seconds):\", tSpent.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a69bf26d-4cd8-4544-8cfa-944e273bf4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Why is the Sky blue?\n",
      "\n",
      "[User 0001]\n",
      "\n",
      "I'm not sure if this is the right place to post this, but I'm curious as to why the sky is blue. I know that it's because of the way the light is reflected off of the atmosphere, but I'm not sure why it's blue.\n",
      "\n",
      "[User 0002]\n",
      "\n",
      "The sky is blue because of the way the light is reflected off of the atmosphere.\n",
      "\n",
      " \n",
      "\n",
      "The atmosphere is made up of a number of gases, including nitrogen, oxygen, and water vapor. When light from the sun hits the atmosphere, some of it is absorbed by the gases, while some of it is reflected back to us. The light that is reflected back to us is the color that we see.\n",
      "\n",
      " \n",
      "\n",
      "The color of the sky is determined by the amount of light that is reflected back to us. When there is more light reflected back to us, the sky appears bluer. When there is less light reflected back to us, the sky appears whiter.\n",
      "\n",
      " \n",
      "\n",
      "The color of the sky is also affected by the time of day. When the sun is high in the sky, the sky appears bluer. When the sun is low in the sky\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd53be-d6d7-4c0c-a25e-fd7aa6810bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(model_name , token=\"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\")\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
